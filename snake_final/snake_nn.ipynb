{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the neural network\n",
    "def build_model(input_size, output_size, learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=input_size, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(output_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\murta\\miniconda3\\envs\\RILearning\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Constants for the board and Q-learning\n",
    "grid_size = (10, 10)  # 10x10 grid\n",
    "food_position = (9, 9)\n",
    "MATRIX_SIZE = 10\n",
    "Q = {}\n",
    "ALPHA = 0.1  # learning rate\n",
    "GAMMA = 0.9  # discount factor\n",
    "EPSILON = 0.2  # for epsilon-greedy strategy\n",
    "\n",
    "\"\"\"===================================================================================\"\"\"\n",
    "# Initialize network and memory buffer\n",
    "state_size = 2  # because the state is given by (x, y) coordinates\n",
    "action_size = 4  # possible actions: up, down, left, right\n",
    "model = build_model(state_size, action_size)\n",
    "target_model = build_model(state_size, action_size)\n",
    "target_model.set_weights(model.get_weights())\n",
    "memory = deque(maxlen=2000)\n",
    "batch_size = 32\n",
    "update_every = 5  # update target network weights every 5 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_coordinates(size):\n",
    "    return (random.randint(0, size - 1), random.randint(0, size - 1))\n",
    "\n",
    "def render_game_board(board_position, score, episode):\n",
    "    \"\"\"This function takes the specified position of the snake, score values and episode\n",
    "    and renders the board for the plot.\n",
    "\n",
    "    Args:\n",
    "        board_position (tuple): poisiton vectors passed into the function\n",
    "        score (int): score\n",
    "        episode (int): number of epochs being run\n",
    "    \"\"\"\n",
    "    # Load and display the game board\n",
    "    board_img = mpimg.imread('board.png')\n",
    "    # Path to the robot icon image\n",
    "    snake_icon_head = 'snake_head.png'\n",
    "    food_icon = 'food.png'\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(board_img)\n",
    "    # Grid size and cell size calculation\n",
    "    cell_size_x = board_img.shape[1] / grid_size[1]\n",
    "    cell_size_y = board_img.shape[0] / grid_size[0]\n",
    "    # Calculate snake icon position (assuming bottom left is (0,0))\n",
    "    snake_x = board_position[0] * cell_size_x + cell_size_x / 2\n",
    "    snake_y = (grid_size[1] - board_position[1] - 1) * cell_size_y + cell_size_y / 2\n",
    "    #Calculate the food position\n",
    "    food_x = food_position[0] * cell_size_x + cell_size_x / 2\n",
    "    food_y = (grid_size[1] - food_position[1] - 1) * cell_size_y + cell_size_y / 2\n",
    "    # # Load and overlay the snake icon\n",
    "    snake_icon = mpimg.imread(snake_icon_head)\n",
    "    food_icon = mpimg.imread(food_icon)\n",
    "    zoom_factor = 0.50  # Adjust zoom factor to fit the icon in the cell\n",
    "    # get the image of the snake\n",
    "    snake_imagebox = OffsetImage(snake_icon, zoom=zoom_factor)\n",
    "    snake_ab = AnnotationBbox(snake_imagebox, (snake_x, snake_y), frameon=False)\n",
    "    # get the image of the mouse\n",
    "    food_imagebox = OffsetImage(food_icon, zoom =zoom_factor)\n",
    "    food_ab = AnnotationBbox(food_imagebox, (food_x, food_y), frameon=False)\n",
    "    #display the images on the graph.\n",
    "    ax.add_artist(snake_ab)\n",
    "    ax.add_artist(food_ab)\n",
    "    # add an x axix label\n",
    "    ax.set_xlabel(f\"Total Reward: {score} | Epoch: {episode}\", fontsize=12, color='blue')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def initialize_q_table(Q):\n",
    "    \"\"\"creates the Q table based on the actions\n",
    "\n",
    "    Args:\n",
    "        Q (dict): this initializes the Q table list that takes an empty dictionary\n",
    "    \"\"\"\n",
    "    for x in range(MATRIX_SIZE):\n",
    "        for y in range(MATRIX_SIZE):\n",
    "            for action in range(4):  # four possible actions\n",
    "                Q[((x, y), action)] = 0\n",
    "\n",
    "def choose_action(state, epsilon=0.2):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randint(0, action_size -1)\n",
    "    state = np.array([state])\n",
    "    q_values = model.predict(state)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# def update_q_value(prev_state, action, reward, next_state, Q):\n",
    "#     \"\"\"this function takes a set of inputs and updates the Q table\n",
    "\n",
    "#     Args:\n",
    "#         prev_state (tuple): position of the snake.\n",
    "#         action (int): the next direction to take for the snake.\n",
    "#         reward (int): the score value\n",
    "#         next_state (tuple): the next calculated position\n",
    "#         Q (list): Q list of the table.\n",
    "#     \"\"\"\n",
    "#     max_future_q = max([Q[(next_state, a)] for a in range(4)])\n",
    "#     current_q = Q[(prev_state, action)]\n",
    "#     # Q-learning formula\n",
    "#     new_q = (1 - ALPHA) * current_q + ALPHA * (reward + GAMMA * max_future_q)\n",
    "#     Q[(prev_state, action)] = new_q\n",
    "\n",
    "def replay():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.array([i[0] for i in minibatch])\n",
    "    actions = np.array([i[1] for i in minibatch])\n",
    "    rewards = np.array([i[2] for i in minibatch])\n",
    "    next_states = np.array([i[3] for i in minibatch])\n",
    "    dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "    target = rewards + GAMMA * np.amax(target_model.predict(next_states), axis=1) * (1 - dones)\n",
    "    target_full = model.predict(states)\n",
    "    target_full[np.arange(batch_size), actions] = target\n",
    "\n",
    "    model.fit(states, target_full, epochs=1, verbose=0)\n",
    "\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def manhattan_distance(point1,point2):\n",
    "    \"\"\"calculates the distance between point A and B.\n",
    "\n",
    "    Args:\n",
    "        point1 (tuple): x,y cooridnates of the snake\n",
    "        point2 (tuple): x,y cooridnates of the snake\n",
    "\n",
    "    Returns:\n",
    "        int: the manhattan distance between the two points\n",
    "    \"\"\"\n",
    "    return abs(point1[0] - point2[0]) + abs(point1[1] - point2[1])\n",
    "\n",
    "def get_reward(old_position, food_position, action):\n",
    "    \"\"\"this function takes the poisiton tuples and returns the next\n",
    "    action to take as well as the reward value\n",
    "\n",
    "    Args:\n",
    "        old_position (tuple): old position of the snake\n",
    "        food_position (tuple): position of the mouse\n",
    "        action (int): direction value\n",
    "\n",
    "    Returns:\n",
    "        tuple: the new x,y coordinate to take\n",
    "        reward: the reward value\n",
    "    \"\"\"\n",
    "    # Set of possible actions\n",
    "    action_space = {\n",
    "        0:(-1,0), # Move left\n",
    "        1:(1,0), # Move right\n",
    "        2:(0,1), # Move up\n",
    "        3:(0,-1), # Move down\n",
    "    }\n",
    "    # calculate the next position\n",
    "    new_position = (old_position[0]+action_space[action][0],\n",
    "                    old_position[1]+action_space[action][1])\n",
    "    \n",
    "    # ensure the new_position is within the board boundry\n",
    "    new_position = (max(0, min(new_position[0], grid_size[0]-1)),\n",
    "                    max(0, min(new_position[1], grid_size[1]-1)))\n",
    "    \n",
    "    old_step = manhattan_distance(old_position, food_position)\n",
    "    new_step = manhattan_distance(new_position, food_position)\n",
    "  \n",
    "    if new_position == food_position:\n",
    "        reward = 10  # reward for reaching the food\n",
    "    elif new_step < old_step:\n",
    "        reward = 4\n",
    "    elif new_step > old_step:\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 0  # small penalty for each move\n",
    "    return new_position, reward\n",
    "\n",
    "initialize_q_table(Q)  # Initialize the Q-table\n",
    "\n",
    "\n",
    "for episode in range(20): #creating the loop based on the number of epochs\n",
    "    pos = (0,0)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    #food_position = generate_random_coordinates(MATRIX_SIZE)\n",
    "    food_position = (9,9)\n",
    "    while pos != food_position and steps < 50: #Looping the code to run but limiting to 50 so it does not go into an infinite loop\n",
    "        display.clear_output(wait=True) #Removes the old graph\n",
    "        render_game_board(pos, total_reward, episode) #Function renders the graph\n",
    "        action = choose_action(pos, EPSILON) #Decides the next action to take\n",
    "        new_position, reward = get_reward(pos, food_position, action) #got the reward\n",
    "        done = new_position == food_position\n",
    "        memory.append((np.array(pos), action, reward, np.array(new_position), done))\n",
    "        replay()\n",
    "\n",
    "        # update_q_value(pos, action, reward, new_position, Q) #updating the Q values \n",
    "        pos = new_position\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        print(f\"Number of Steps: \", {steps})\n",
    "\n",
    "    if episode % update_every == 0:\n",
    "        update_target_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RILearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
