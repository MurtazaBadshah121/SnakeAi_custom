{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary packages.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import random\n",
    "from IPython import display\n",
    "\n",
    "# Constants for the board and Q-learning\n",
    "grid_size = (10, 10)  # 10x10 grid\n",
    "food_position = (9, 9)\n",
    "MATRIX_SIZE = 10\n",
    "Q = {}\n",
    "ALPHA = 0.1  # learning rate\n",
    "GAMMA = 0.9  # discount factor\n",
    "EPSILON = 0.2  # for epsilon-greedy strategy\n",
    "\n",
    "def render_game_board(board_position, score, episode):\n",
    "    \"\"\"This function takes the specified position of the snake, score values and episode\n",
    "    and renders the board for the plot.\n",
    "\n",
    "    Args:\n",
    "        board_position (tuple): poisiton vectors passed into the function\n",
    "        score (int): score\n",
    "        episode (int): number of epochs being run\n",
    "    \"\"\"\n",
    "    # Load and display the game board\n",
    "    board_img = mpimg.imread('board.png')\n",
    "    # Path to the robot icon image\n",
    "    snake_icon_head = 'snake_head.png'\n",
    "    food_icon = 'food.png'\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(board_img)\n",
    "    # Grid size and cell size calculation\n",
    "    cell_size_x = board_img.shape[1] / grid_size[1]\n",
    "    cell_size_y = board_img.shape[0] / grid_size[0]\n",
    "    # Calculate snake icon position (assuming bottom left is (0,0))\n",
    "    snake_x = board_position[0] * cell_size_x + cell_size_x / 2\n",
    "    snake_y = (grid_size[1] - board_position[1] - 1) * cell_size_y + cell_size_y / 2\n",
    "    #Calculate the food position\n",
    "    food_x = food_position[0] * cell_size_x + cell_size_x / 2\n",
    "    food_y = (grid_size[1] - food_position[1] - 1) * cell_size_y + cell_size_y / 2\n",
    "    # # Load and overlay the snake icon\n",
    "    snake_icon = mpimg.imread(snake_icon_head)\n",
    "    food_icon = mpimg.imread(food_icon)\n",
    "    zoom_factor = 0.50  # Adjust zoom factor to fit the icon in the cell\n",
    "    # get the image of the snake\n",
    "    snake_imagebox = OffsetImage(snake_icon, zoom=zoom_factor)\n",
    "    snake_ab = AnnotationBbox(snake_imagebox, (snake_x, snake_y), frameon=False)\n",
    "    # get the image of the mouse\n",
    "    food_imagebox = OffsetImage(food_icon, zoom =zoom_factor)\n",
    "    food_ab = AnnotationBbox(food_imagebox, (food_x, food_y), frameon=False)\n",
    "    #display the images on the graph.\n",
    "    ax.add_artist(snake_ab)\n",
    "    ax.add_artist(food_ab)\n",
    "    # add an x axix label\n",
    "    ax.set_xlabel(f\"Score: {score} | epoch: {episode}\", fontsize=12, color='blue')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def initialize_q_table(Q):\n",
    "    \"\"\"creates the Q table based on the actions\n",
    "\n",
    "    Args:\n",
    "        Q (dict): this initializes the Q table list that takes an empty dictionary\n",
    "    \"\"\"\n",
    "    for x in range(MATRIX_SIZE):\n",
    "        for y in range(MATRIX_SIZE):\n",
    "            for action in range(4):  # four possible actions\n",
    "                Q[((x, y), action)] = 0\n",
    "\n",
    "def choose_action(state, Q, epsilon=0.2):\n",
    "    \"\"\"this function helps the code to decide the next best action\n",
    "\n",
    "    Args:\n",
    "        state (tuple): this is the initial starting point or position of the snake\n",
    "        Q (dict): this is the dict where we retrieve the MAX value for direction\n",
    "        epsilon (float, optional): _description_. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)  # explore: choose a random action\n",
    "    else:\n",
    "        # exploit: choose the best action based on current Q-values\n",
    "        actions = [Q[(state, action)] for action in range(4)]\n",
    "        return actions.index(max(actions))\n",
    "\n",
    "def update_q_value(prev_state, action, reward, next_state, Q):\n",
    "    max_future_q = max([Q[(next_state, a)] for a in range(4)])\n",
    "    current_q = Q[(prev_state, action)]\n",
    "    # Q-learning formula\n",
    "    new_q = (1 - ALPHA) * current_q + ALPHA * (reward + GAMMA * max_future_q)\n",
    "    Q[(prev_state, action)] = new_q\n",
    "\n",
    "def manhattan_distance(point1,point2):\n",
    "    return abs(point1[0] - point2[0]) + abs(point1[1] - point2[1])\n",
    "\n",
    "def get_reward(old_position, food_position, action):\n",
    "    # Set of possible actions\n",
    "    action_space = {\n",
    "        0:(-1,0), # Move left\n",
    "        1:(1,0), # Move right\n",
    "        2:(0,1), # Move up\n",
    "        3:(0,-1), # Move down\n",
    "    }\n",
    "    # calculate the next position\n",
    "    new_position = (old_position[0]+action_space[action][0],\n",
    "                    old_position[1]+action_space[action][1])\n",
    "    \n",
    "    # ensure the new_position is within the board boundry\n",
    "    new_position = (max(0, min(new_position[0], grid_size[0]-1)),\n",
    "                    max(0, min(new_position[1], grid_size[1]-1)))\n",
    "    \n",
    "    old_step = manhattan_distance(old_position, food_position)\n",
    "    new_step = manhattan_distance(new_position, food_position)\n",
    "  \n",
    "    if new_position == food_position:\n",
    "        reward = 10  # reward for reaching the food\n",
    "    elif new_step < old_step:\n",
    "        reward = 4\n",
    "    elif new_step > old_step:\n",
    "        reward = -1\n",
    "    else:\n",
    "        reward = 0  # small penalty for each move\n",
    "    return new_position, reward\n",
    "\n",
    "initialize_q_table(Q)  # Initialize the Q-table\n",
    "\n",
    "for episode in range(500):\n",
    "    pos = (0,0)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while pos != food_position and steps < 50:\n",
    "        display.clear_output(wait=True)\n",
    "        render_game_board(pos, total_reward, episode)\n",
    "        action = choose_action(pos, Q, EPSILON)\n",
    "        new_position, reward = get_reward(pos, food_position, action)\n",
    "        update_q_value(pos, action, reward, new_position, Q)\n",
    "        pos = new_position\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        print(f\"Number of Steps: \", {steps})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action = choose_action(pos, Q, EPSILON)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RILearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
